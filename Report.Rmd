---
title: "Forecasting the price of Italian Red Wine"
author: "Viktoriia Ilina"
date: "July 2021"
output:
  bookdown::pdf_document2:
    latex_engine: lualatex
  tufte::tufte_handout:
    latex_engine: xelatex
geometry: left=2cm,right=2cm,top=2cm,bottom=2cm
---

# Overview

Life is too short to drink bad wine - Johann Wolfgang von Goethe

Tuscany, the Promised Land for every wine devotee, and an ultimate destination of any path seeking best winy experience. A mesmerizing landscape full of tiny hilltop villages wrapped around by winding roads, monumental cypress trees and ambrosial vineyards. Indeed, a land made of folk tales and fables. And the spice, passion, scent, and vitality of wine is the essence that nourishes its soul with the mysterious power of Tuscan art of winemaking.  Zeffiro Ciuffoletti, a renown historian, once came up with a perfect definition of the evolvement of Tuscan wine, saying: “Tuscany, in regards to wines, has no equal in the whole world, thanks to generous nature, and to a civilization of grapevine and of wine that has been decanted and refined over centuries[^1].”  Italy’s most fruitful winery area, Tuscany is rightly glorious by its master reference, law-backed wines including 41 DOC (Denomination of Controlled Origin) and 11 DOCG (Denomination of Controlled and Guaranteed Origin), a category, that only Italy’s best wines get rewarded with. Besides, there are further six of more flexible designations IGP/IGT, with the pan-regional Toscana IGP[^2]. But, how does one find out the way to not get lost among all this diversity? The reading of wine label can be undoubtedly confusing and overwhelming for the layman.  American oak, French oak, Hungarian oak, stainless steel, concrete, new, neutral, 15% of one and 85% of another…. What the heck does this all mean? And which factors determine the price of wine?

The main goal of this project is trying to make a robust prediction model of the price of a bottle of wine based on the data indicated on its label (name, producer, variety of grape, classification, alcohol content, year and type of aging) . The data was extracted from  xtraWine, one of the best wine-shop in Italy 2021 by Gambero Rosso[^3]. The dataset includes information on 803 red wines produced by  243  firms in the Tuscany region with a volume of 0.75 liters. All prices are shown in euros (VAT included). This report will present an exploratory analysis of the data, methodology and training the models, results and discussion.

[^1]: https://timelessitalytravels.com/2017/06/26/an-intriguing-history-of-tuscan-wine/

[^2]: https://www.wine-searcher.com/regions-tuscany

[^3]: https://www.gamberorosso.it/ristoranti/scheda-enoteca/xtrawine/

# Exploratory data analysis

```{r Loading required packages, eval = TRUE, include = F}

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(lattice)) install.packages("lattice", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(caTools)) install.packages("caTools", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
if(!require(class)) install.packages("class", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(mltools)) install.packages("mltools", repos = "http://cran.us.r-project.org")


library(tidyverse)
library(dplyr)
library(data.table)
library(readxl)
library(lattice)
library(ggplot2)
library(caTools)
library(caret)
library(e1071)
library(class)
library(xgboost)
library(mltools)

```


```{r Importing data, eval = TRUE, include = F}

# Important pre-step: download the file to your working directory!

my_data <- read_excel("Red.xlsx")

```

First of all, let's take a quick look at the data. 

```{r  Getting the first 5 rows, echo = FALSE, eval = TRUE}

head(my_data)

```

For further analysis, delete irrelevant column “Name” and convert other columns' content to the proper format. Thereafter, check our dataset.

```{r Data pre-processing, echo = FALSE, eval = TRUE}

# Deleting irrelevant column

my_data <- my_data[-1]

# Transforming the data

my_data <- transform(
  my_data,
  Producer = as.factor(Producer),
  Grape = as.factor(Grape),
  Classification = as.factor(Classification),
  Alcohol = as.numeric(Alcohol),
  Year = as.factor(Year),
  Aging = as.factor(Aging),
  Price = as.numeric(Price)
)

# Checking the result

summary(my_data)

```

The distribution of bottle prices in the dataset is right-skewed, with most wines costing less than 50 euros, but with a long right tail of much more expensive wines (the highest price is 324.52 euro).

```{r Price distribution, echo = FALSE, eval = TRUE, fig.align = "center", out.width = "70%"}

histogram(my_data$Price, type = "count", 
  main = "Distribution of bottle prices", 
  xlab = "Price in euros",
  col = "gold2")

```

The Italian wine making sector is characterised by a high number of small or mid-sized players, mostly cooperatives or family owned companies[^4], so it is not surprising that 58 producers have just one wine bottle, while the maximum count of the bottles belongs to two brends (Banfi и Cantine Leonardo da Vinci) - 19 each. 

[^4]: https://www.lexology.com/library/detail.aspx?g=dc3ec3b9-ef49-4f6a-9f56-b0ec873815aa

```{r Information about producers, echo = FALSE, eval = FALSE}

Producers <- my_data %>% 
  group_by(Producer) %>%
  summarize(Count = n()) %>%
  arrange(desc(Count)) %>% 
  print()

sum(with(Producers, Count == 1))

```

87% of Tuscan wine is red with a staggering dominance of the local star Sangiovese (63% of the total vineyard surface area)[^5].  So, it’s quite natural that in the dataset most of introduced wines contain or are fully made of this variety.

[^5]: https://www.vivino.com/wine-news/everything-you-need-to-know-about-tuscan-wine-in-600-words

```{r Distribution of grape variety, echo = FALSE, eval = TRUE, fig.align = "center", out.width = "85%", message = FALSE, warning = FALSE}

Grape_variety <- my_data %>% 
  group_by(Grape) %>%
  summarize(Count = n()) 

ggplot(data = Grape_variety, aes(x = reorder(Grape, Count), y = Count)) + geom_bar(stat = 'identity', aes(fill = Count)) + coord_flip() + theme_classic() + scale_fill_gradient(low = 'gold', high = 'gold3') + labs(title = 'Grape variety distribution', x = 'Variety of grape', y = 'Number of bottles')
```

69% of wines in the compilation have an alcohol content of at least 14% ABV,  which proves the popular assumption of wine being a low alcohol drink, wrong. According to Liv-ex (the global marketplace for the wine trade), alcohol in wine is rising - red wines from Tuscany had higher alcohol levels on average in the decade between 2010 and 2019 than they did in 1990s[^6]. This is facilitated by the climate changes and special techniques aimed to encourage ripening(planting at higher densities with low-yielding clones, short pruning, green harvesting and so on)[^7]. 

[^6]: https://www.decanter.com/learn/are-alcohol-levels-in-wine-rising-data-460879/
[^7]: https://www.winemag.com/2015/04/23/hot-italian-wines-is-15-abv-the-new-14/

```{r High alcohol content, echo = FALSE, eval = FALSE}

sum(my_data$Alcohol >= 14)/nrow(my_data)

```

Most of the wines in the selection have a vintage from 2015 to 2019.

```{r Wines by year, echo = FALSE, eval = TRUE}

Vintage <- my_data %>% 
  group_by(Year) %>%
  summarize(Count = n()) %>%
  arrange(desc(Count)) %>% 
  print()

```

As depicted in the chart below, overall, the younger wines are cheaper than their vintage kindred. 

```{r Median wine price by grape harvest, echo = FALSE, eval = TRUE, fig.align = "center", out.width = "85%", message = FALSE, warning = FALSE}

ggplot(data = my_data) + geom_boxplot(mapping = aes(x = reorder(Year, Price, FUN = median), y = Price), fill = 'gold') + theme_classic() + labs(title = 'Median wine price by grape harvest', x = 'Vintage', y = 'Price')

```

Aging is the phase between alcoholic fermentation and bottling allowing the production of a wine with more rich and complex organoleptic qualities.
Type of container directly affects the result of aging , along with oxygen, temperature, humidity, light and keeping practices, such as topping ups and rackings[^8]. Below we can see an obvious interconnection between kind of aging and price of the bottle.

[^8]: http://www.diwinetaste.com/dwt/en2007066.php

```{r Median wine price by kind of aging, echo = FALSE, eval = TRUE, fig.align = "center", out.width = "85%", message = FALSE, warning = FALSE}

ggplot(data = my_data) + geom_boxplot(mapping = aes(x = reorder(Aging, Price, FUN = median), y = Price), fill = 'gold') + theme_classic() + labs(title = 'Median wine price by kind of aging', x = 'Type of aging', y = 'Price')

```

# Methodology and analysis

Non-symmetric distribution of bottle prices with horde of outliers in the dataset may be the reason of bad performance of the models. So, It might make sense to categorize a continuous variable using several data independent cut-offs and reclassify the problem as the multi-class (multinomial) classification. Common machine learning algorithms that can be used for this task include: k-Nearest Neighbors, Decision Trees, Naïve Bayes, Random Forest, Gradient Boosting, Artificial neural networks and so on. Below we will use three of them.

Go through our transformed dataset:

```{r Categorization of price data, echo = FALSE, eval = TRUE}

# Creating categorical variable based on range and removing original data

my_data$Category <- cut(my_data$Price, c(0, 10, 25, 50, 75, 100, 200, 325))
levels(my_data$Category) <- c("<10", "10-25", "25-50", "50-75", "75-100", "100-200", ">200")
my_data$Price <- NULL

# For further analysis, convert "Alcohol" column content to factor

my_data$Alcohol <- as.factor(my_data$Alcohol)

# Checking the dataset

head(my_data)

```

Before proceeding to the models and algorithms, we should split our dataset into training (80%) and testing (20%) sets.

```{r Splitting the data, echo = FALSE, eval = TRUE}

set.seed(2) # set the seed to make the partion reproducible

sample <- sample.split(my_data$Category, SplitRatio = .8)
train <- subset(my_data, sample == TRUE)
test <- subset(my_data, sample == FALSE)


```

To get the base accuracy of the dataset, we will use a very popular supervised classification algorithm - Naive Bayes. This algorithm is based on the Bayes' theorem, that describes the probability of an event based on prior knowledge of the conditions that might be relevant to the event[^9]. The Bayes' rule can be expressed in the following formula:

$$P(A|B) = \frac {P(B|A)*P(A)} {P(B)}$$

Where:

-   $P(A|B)$ is the probability of event A occuring, given event B has occured;
-   $P(B|A)$ is the probability of event B occuring, given event A has occured;
-   $P(A)$ is the probability of event A;
-   $P(B)$ is the probability of event B.

[^9]: https://corporatefinanceinstitute.com/resources/knowledge/other/bayes-theorem/

Train and test the model:

```{r Naive Bayes model, echo = TRUE, eval = TRUE}

set.seed(120) # set the seed to make the prediction reproducible

# Fitting the model

model_naive <- naiveBayes(Category ~ ., data = train)
 
# Predicting on test data

prediction_naive <- predict(model_naive, newdata = test)
 
# Model evaluation

confusionMatrix(test$Category, prediction_naive)

```

The model achieved 62.73% accuracy with a p-value close to 0. It's better than a random choice, nevertheless very far from ideal. 

Let's try to achieve more adequate predictions using an another model - Support Vector Machines(SVM). The objective of SVM is to find a hyperplane that maximizes the separation of the data points to their actual classes in an n-dimensional space. The data points which are at the minimum distance to the hyperplane i.e, closest points are called Support Vectors[^10]. For fitting of this model we should choose two hypermeters: "C" for controlling error and "Gamma" for giving curvature weight of the decision boundary.

[^10]: https://www.analyticsvidhya.com/blog/2021/05/multiclass-classification-using-svm/

Here:

```{r Support Vector Machines model, echo = TRUE, eval = TRUE}

set.seed(2) # set the seed to make the prediction reproducible

# Training the model

model_svm <- svm(Category ~ ., data=train, 
  method="C-classification", kernal="radial", 
  gamma=0.1, cost=3)

# Test model on test data

prediction_svm <- predict(model_svm, test)
 
# Model evaluation

confusionMatrix(test$Category, prediction_svm)

```

Well, forecast accuracy has increased by 5%. Unfortunately. it's still a different result from what had been expected. 

The last model that we'll try to implement is eXtreme Gradient Boosting(XGB). XGB is an efficient open-source application of the stochastic gradient boosting ensemble algorithm[^11]. This algorithm found widespread use due to memory efficiency, time saving and model perfomance. 

[^11]: https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/

```{r eXtreme Gradient Boosting model, echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE}

# XGBoost requires the classes to be in an integer format, starting with 0. 
# So, we have to convert "Category" factor to the proper format 

Categories <- my_data$Category
label <- as.integer(my_data$Category)-1
my_data$Category <- NULL

#Encoding all other variables to dummy variables

my_data <- one_hot(as.data.table(my_data))

# Splitting the data for training and testing

set.seed(2) # set the seed to make the partion reproducible

n <- nrow(my_data)
index <- sample(n, floor(0.8*n))
train <- as.matrix(my_data[index,])
train_label <- label[index]
test <- as.matrix(my_data[-index,])
test_label <- label[-index]

# Creating the xgb.DMatrix objects

xgb.train <- xgb.DMatrix(data = train,label = train_label)
xgb.test <- xgb.DMatrix(data = test,label = test_label)

# Define the parameters for multinomial classification

num_class <- length(levels(Categories))
xgb_params <- list(
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class
)

set.seed(120) # set the seed to make the prediction reproducible

# Training the XGBoost model

xgb.fit <- xgb.train(
  params = xgb_params,
  data = xgb.train,
  nrounds = 100,
  early_stopping_rounds = 10,
  watchlist = list(val1 = xgb.train,val2 = xgb.test),
  verbose = 0
)

# Prediction on test data

xgb.pred <- predict(xgb.fit, test, reshape=T)
xgb.pred <- as.data.frame(xgb.pred)
colnames(xgb.pred) <- levels(Categories)

xgb.pred$prediction <- apply(xgb.pred, 1, function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label <- levels(Categories)[test_label+1]

# Model evaluation

confusionMatrix(factor(xgb.pred$prediction),
                factor(xgb.pred$label),
                mode = "everything")

```

Surprisingly, this algorithm performed the worst prediction power.

# Results and discussion

The main goal of this project was trying to make a robust prediction model of the price of a bottle of wine based on the data indicated on its label. Unfortunately, we must acknowledge that this attempt failed. The best percentage of correct predictions for the test data, driven by Support Vector Machines algorithm, only managed to hit the mark of 67%, which is a barely feasible result. Thus, label information is not reliable predictor of bottle price.

However, future work could likely improve predictive performance in the following ways:

-   expanding the sample beyond one region to the whole country or a number of countries;
-   trying to solve this problem as a regression task;
-   exploring more factors such as awards, packaging, reviews of the sommeliers, number of produced bottles, philosophy(organic/no), ageing potential and so on;
-   evaluating the performance of the model via Accuracy, Precision, Recall and F1 Score metrics; 
-   using other machine learning algorithms (e.g. KNN).

